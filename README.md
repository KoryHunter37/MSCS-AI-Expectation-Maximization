# MSCS Project

This repository overviews an Artificial Intelligence design project which I completed while completing my Master's in Computer Science at Georgia Tech. The source code for this project is not publically available, in compliance with the Georgia Institute of Technology Academic Honor Code<sup><a href="https://policylibrary.gatech.edu/student-affairs/academic-honor-code">[1]</a></sup>.

Instead, I have substituted my raw code with a guided walkthrough of the project itself, the steps I went through to complete it, and the skills I acquired by solving it.

# Overview

Automatic image processing is a key component to many AI systems, including facial recognition and video compression. One basic method for processing is segmentation, by which one divides an image into a fixed number of components in order to simplify its representation. For example, I can train a mixture of Gaussians to represent an image, and segment it according to the simplified representation as shown in the images below.

<p align="center"><img width="370" height="208" src=images/self-driving-1.png></img></p>
<div align="center"><b>Fig 1. Footage from a Self-Driving Car</b></div>

<p align="center"><img width="370" height="208" src=images/self-driving-2.png></img></p>
<div align="center"><b>Fig 2. Simplified Footage</b></div>

In this image, I learned how to perform image segmentation by implemenenting Guassian mixture models and iteratively improving their performance. I implemented several methods of image segmentation, with increasing complexity:

1. Implemented k-means clustering to segment a color image.
2. Built a Gaussian mixture model to be trained with expectation-maximization.
3. Experimented with varying the details of the Gaussian mixture model’s implementation.
4. Implemented and tested a new metric called the Bayesian information criterion, which guaranteed a more robust image segmentation.

# Gaussian Mixture Models

A Gaussian mixture model is a generative model<sup><a href="https://en.wikipedia.org/wiki/Generative_model">[2]</a></sup> for representing the underlying probability distribution of a complex collection of data, such as the collection of pixels in a grayscale photograph.

In the context of this problem, a Gaussian mixture model defines the joint probability f(x) as:

<p align="center"><img width="374" height="122" src=images/formula-1.png></img></p>

* x is a grayscale value [0,1]
* f(x) is the joint probability of that gray scale value
* m<sub>i</sub> is the mixing coefficient on component i 
* N<sub>i</sub> is the i<sup>th</sup> Gaussian distribution underlying the value x with mean μ<sub>i</sub> and variance σ<sub>i</sub><sup>2</sup>

I used this model to segment photographs into different grayscale regions. The idea of segmentation is to assign a component i to each pixel x using a maximum posterior probability, which equates to:

<p align="center"><img width="562" height="78" src=images/formula-2.png></img></p>

Then I replaced each pixel in the image with its corresponding μ<sub>i</sub>, to produce a result similar to what I showed in Figure 2.

# K-means Clustering

One easy method for image segmentation is to simply cluster all similar data points together and then replace their values with the mean value. Thus, I began using k-means clustering<sup><a href="https://en.wikipedia.org/wiki/K-means_clustering">[3]</a></sup> to find a foothold in this project. This also provided a baseline to compare with my segmentation later on.

I created a function to convert the original image values matrix to its clustered counterpart. My convergence test was essentially to see if the assigned clusters stopped changing non-trivially between iterations of clustering. This test was rather slow, but I expected it to be.

In some circumstances, no initial cluster means were provided for me. In those instances, my function chose k random points from the data (without replacement) to use as cluster means.

Since clustering is best used on multidimensional data, I was tasked with now analyzing a color image as seen below.

<p align="center"><img width="366" height="270" src=images/bird.png></img></p>
<div align="center"><b>Fig 3. Color Bird Photograph</b></div>
<br/><br/>

I clustered this image with a variety of different cluster counts. Here are the results I generated by clustering this image into 2, 3, 4, 5, and 6 different segments. (Always starting from the original stock image, of course!)

<p align="center"><img width="366" height="270" src=images/bird-k2.png></img></p>
<div align="center"><b>Fig 4. K-2 Color Bird Photograph</b></div>
<br/><br/>
<br/><br/>

<p align="center"><img width="366" height="270" src=images/bird-k3.png></img></p>
<div align="center"><b>Fig 5. K-3 Color Bird Photograph</b></div>
<br/><br/><br/><br/>

<p align="center"><img width="366" height="270" src=images/bird-k4.png></img></p>
<div align="center"><b>Fig 6. K-4 Color Bird Photograph</b></div>
<br/><br/>
<br/><br/>

<p align="center"><img width="366" height="270" src=images/bird-k5.png></img></p>
<div align="center"><b>Fig 7. K-5 Color Bird Photograph</b></div>
<br/><br/>
<br/><br/>

<p align="center"><img width="366" height="270" src=images/bird-k6.png></img></p>
<div align="center"><b>Fig 8. K-6 Color Bird Photograph</b></div>
<br/><br/>
<br/><br/>

# Implementing a Gaussian Mixture Model

Next, I stepped beyond clustering and implemented a complete Gaussian mixture model.

I began this process by calculating the joint log probability of a given greyscale value. (Same formula as previously provided.)

I used my own custom implementation of expectation maximization<sup><a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm">[4]</a></sup> to train the model to represent the image as a mixture of Gaussians. To initialize the expectation maximization process, I set each component's mean to the grayscale value of a randomly chosen pixel. My variance was set to 1.0, and the mixing coefficients were a uniform distribution.

The convergence conditions I used were simply a test of, "If the new likelihood is within 10% of the previous likeliehood for 10 consecutive iterations, the model has converged."

After that, I calculated the log likelihood of the trained model, and segmented the image according to the trained model. The best segmentation was determined by iterating over the model, training and scoring, since expectation maximization isn't guaranteed to converge to the global maximum (it can ge easily caught by local maximums / outliers).

### Log Probability Hangup

I began noticing a recurring issue where, due to the multiplication of lots of probabilities in a sequence, I ended up with a probability of zero due to underflow. To avoid this, I decided to switch all probability calculations for the rest of the assignment to log probabilities.

The log form of the Gaussian probability of scalar value x is:

<p align="center"><img width="324" height="41" src=images/formula-3.png></img></p>

* m<sub>u</sub> is the standard deviation

I had a much easier time working with my data after implementing this, and shifting to using numpy arrays to store my data instead of trying to use standard lists of lists. This was especially important for vectorizing my code, where I outsourced some of my calculations to faster programming languages, then returned those values back into python.

# Model Experimentation

I tried a few methods to improve my GMM performance.

### Improved Initialization

To run expectation maximization in my baseline Gaussian mixture model, I used a random initialization to determine the initial values for my component means. I was pretty confident that I could do better than this!

I implemented an improved Gaussion mixture model into my component initialization, and while this slowed down initialization a bit, it really helped once the process began running.

## Convergence Condition

I was also a bit skeptical of the previously used convergence condition. I began analyzing each of the new model parameters (means, variances, and mixing coefficients), and if ANY of them were within 10% of the previous variables for 10 consecutive iterations, I could consider the analysis converged.

# Bayesian Information Criterion

In my previous solutions, the only criterion for choosing a model was whether it maximized the posterior likelihood-- regardless of how many parameters this required. As a result, the "best" model may simply have been the model with the most parameters... As you can imagine, this was overfitting the training data.

To avoid overfitting, I used Bayesian information criterion<sup><a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">[5]</a></sup>, which penalized models based on the number of parameters they used. As simple as this sounds, it was a massive breakthrough for me!

In the case of the Gaussian mixture model, this was equal to the number of components times the number of variables per component. (ie. mean, variance, and mixing coefficients = 3x components)

The performance of my models on unseen data grew tremendously after this breakthrough, and it was enough to surpass all required success rates for the assignment.

# Reflection

This was the longest assignment I completed in my graduate AI course at Georgia Tech, but it was well worth it. I iteratively reworked various parts of my algorithmic implementations with respect to how I saw the results change and grow.

I learned that sometimes experimentation is truly the only way to progress in a difficult and abstract problem environment, and even though I implemented many algorithms which did not make it into the final version of my project, I feel like I learned something new about the data with each failure.

I felt greatly rewarded for not giving up when I got hungup on my underflow issue, and was reminded that sometimes a problem which appears to plague the whole project can really just be resolved with a simple fundamental fix.
